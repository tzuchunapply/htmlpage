<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hackathon Case Study | Tzu-Chun Huang</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <style>
        :root {
            --bg-color: #f8fafc;
            --primary-dark: #1e293b;
            --accent-blue: #0969da;
            --accent-purple: #7c3aed;
            --accent-green: #059669;
            --text-main: #334155;
            --text-muted: #64748b;
            --white: #ffffff;
            --border-color: #e2e8f0;
        }

        * { margin: 0; padding: 0; box-sizing: border-box; font-family: 'Segoe UI', system-ui, sans-serif; }
        body { background-color: var(--bg-color); color: var(--text-main); line-height: 1.6; padding-bottom: 80px; }

        nav { background: #1a1a1a; padding: 1rem; position: sticky; top: 0; z-index: 1000; }
        .nav-container { max-width: 1000px; margin: 0 auto; }
        .nav-container a { color: #fff; text-decoration: none; font-weight: 600; display: flex; align-items: center; gap: 8px; font-size: 0.9rem; }

        header { max-width: 1000px; margin: 50px auto 30px; padding: 0 25px; }
        .badge { background: #ffc107; color: #000; padding: 5px 15px; font-weight: 800; font-size: 0.75rem; border-radius: 4px; display: inline-block; margin-bottom: 15px; }
        h1 { font-size: 2.2rem; color: var(--primary-dark); line-height: 1.2; }
        .subtitle { font-size: 1.1rem; color: var(--accent-blue); font-weight: 600; margin-top: 10px; }

        .container { max-width: 1000px; margin: 0 auto; padding: 0 25px; }

        .case-section { background: var(--white); border: 1px solid var(--border-color); border-radius: 12px; padding: 35px; margin-bottom: 30px; box-shadow: 0 4px 6px -1px rgba(0,0,0,0.05); }
        h2 { font-size: 1.4rem; margin-bottom: 20px; color: var(--primary-dark); display: flex; align-items: center; gap: 12px; border-bottom: 2px solid var(--bg-color); padding-bottom: 10px; }
        
        .grid-2 { display: grid; grid-template-columns: 1fr 1fr; gap: 35px; align-items: start; }
        
        ul { list-style: none; }
        li { position: relative; padding-left: 20px; margin-bottom: 8px; font-size: 0.95rem; }
        li::before { content: "→"; position: absolute; left: 0; color: var(--accent-blue); font-weight: bold; }

        .figure-container { margin-top: 10px; text-align: center; }
        .figure-container img { 
            width: 100%; 
            height: auto; 
            border-radius: 8px; 
            border: 1px solid var(--border-color);
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        .figure-caption { font-size: 0.8rem; color: var(--text-muted); margin-top: 8px; font-style: italic; }

        @media (max-width: 850px) { .grid-2 { grid-template-columns: 1fr; } }
    </style>
</head>
<body>

<nav>
    <div class="nav-container">
        <a href="https://tzuchunapply.github.io/index.html#projects"><i class="fas fa-arrow-left"></i> BACK TO PORTFOLIO</a>
    </div>
</nav>

<header>
    <span class="badge"><i class="fas fa-trophy"></i> AWARDED 1ST PLACE</span>
    <h1>MoS₂ Defect Identification & Classification</h1>
    <div class="subtitle">Deep Learning Hackathon | Azure ML & YOLOv5 Pipeline</div>
</header>

<div class="container">

    <!-- Section 1: Azure Custom Vision -->
    <div class="case-section">
        <h2><i class="fas fa-eye" style="color: var(--accent-blue);"></i> Phase 1: Azure Custom Vision</h2>
        <div class="grid-2">
            <div>
                <h4>Initial Detection Baseline</h4>
                <p>Established a baseline using <strong>Azure Custom Vision</strong> to rapidly prototype a model capable of localizing structural defects in MoS₂ lattices.</p>
                <ul>
                    <li><strong>ImageJ Integration:</strong> Converted manual labels into Azure-compatible coordinate formats.</li>
                    <li><strong>Training Split:</strong> Implemented an 80/20 train-test split for objective performance evaluation.</li>
                    <li><strong>Resource Management:</strong> Leveraged Azure cloud computing for efficient model iteration.</li>
                </ul>
            </div>
            <div class="figure-container">
                <!-- 依要求使用 MAP.png -->
                <img src="assets/img/MAP.png" alt="Azure Custom Vision Training Metrics">
                <p class="figure-caption">Figure 1: Iteration 1 performance metrics from Azure Custom Vision, showing Precision, Recall, and mAP.</p>
            </div>
        </div>
    </div>

    <!-- Section 2: Advanced Annotation with Roboflow -->
    <div class="case-section">
        <h2><i class="fas fa-tags" style="color: var(--accent-purple);"></i> Phase 2: Multiclass Labeling with Roboflow</h2>
        <div class="grid-2">
            <div>
                <h4>Transition to YOLOv5 Architecture</h4>
                <p>To differentiate specific types of defects, we transitioned to <strong>YOLOv5</strong> and used <strong>Roboflow</strong> for advanced multiclass annotation management.</p>
                <ul>
                    <li><strong>Defect Classification:</strong> Categorized defects into <i>Broken Region</i>, <i>Vacancy</i>, and <i>Replacement</i>.</li>
                    <li><strong>Dataset Management:</strong> Roboflow enabled efficient handling of multiclass labels and exported them directly into YOLOv5 PyTorch format (.txt and .yaml).</li>
                    <li><strong>Annotation Accuracy:</strong> Improved label consistency across the dataset, addressing the complexity of atomic-scale defect features.</li>
                </ul>
            </div>
            <div style="background: #f1f5f9; padding: 20px; border-radius: 8px;">
                <p style="font-size: 0.85rem; font-family: monospace; color: #1e293b;">
                    # data.yaml Example<br>
                    train: ../train/images<br>
                    val: ../valid/images<br>
                    nc: 3<br>
                    names: ['Broken Region', 'Replacement', 'Vacancy']
                </p>
                <p class="figure-caption" style="margin-top: 15px;">Sample of the YOLOv5 configuration file generated via Roboflow.</p>
            </div>
        </div>
    </div>

    <!-- Section 3: Image Preprocessing -->
    <div class="case-section">
        <h2><i class="fas fa-magic" style="color: var(--accent-purple);"></i> Phase 3: Image Preprocessing</h2>
        <div class="grid-2">
            <div>
                <h4>Enhancing Data Quality</h4>
                <p>Applied professional image filters to the raw microscopy data to improve detection of subtle features:</p>
                <ul>
                    <li><strong>Gaussian & Median Filters:</strong> Effectively reduced background noise and instrumentation artifacts.</li>
                    <li><strong>Contrast Optimization:</strong> Adjusted contrast (~8.71) to enhance the visibility of atomic vacancies.</li>
                    <li><strong>"Ice" Color Map:</strong> Provided the most distinct visual contrast for deep learning feature extraction.</li>
                </ul>
            </div>
            <div class="figure-container">
                <img src="assets/img/Image preprocessing.png" alt="Image Preprocessing Comparison">
                <p class="figure-caption">Figure 2: Comparison between raw microscopy data and preprocessed images for enhanced defect visibility.</p>
            </div>
        </div>
    </div>

    <!-- Section 4: Intensity Profiles Verification -->
    <div class="case-section">
        <h2><i class="fas fa-chart-line" style="color: #ea580c;"></i> Phase 4: Technical Verification</h2>
        <div class="grid-2">
            <div class="figure-container">
                <img src="assets/img/Intensity Profile.png" alt="Intensity Profiles Verification">
                <p class="figure-caption">Figure 3: Intensity line profiles confirming model predictions against physical atomic data.</p>
            </div>
            <div>
                <h4>Cross-Referencing Model Predictions</h4>
                <p>Verified deep learning results using <strong>Intensity Line Profiles</strong> (Nion Swift) to ensure physical validity:</p>
                <ul>
                    <li><strong>Broken Regions:</strong> Confirmed by significant and total intensity drops in the profile.</li>
                    <li><strong>Atomic Vacancies:</strong> Verified by periodic, specific intensity dips corresponding to Mo/S atomic positions.</li>
                    <li><strong>Robustness:</strong> This secondary physical verification proved that the YOLOv5 model accurately captured the intended chemical features.</li>
                </ul>
            </div>
        </div>
    </div>

    <!-- Section 5: Generalizability -->
    <div class="case-section" style="border-left: 8px solid var(--accent-green);">
        <h2><i class="fas fa-microscope"></i> Model Generalizability</h2>
        <p>The final model was evaluated using MoS₂ images sourced from peer-reviewed literature (ACS Nano, Nano Letters). It demonstrated a high ability to detect defects across unseen images from different experimental setups, proving its value as a generalized tool for semiconductor and material science inspection.</p>
    </div>

</div>

<footer>
    <div style="text-align: center; color: var(--text-muted); padding: 40px 0;">
        &copy; 2026 Tzu-Chun Huang | Project Group: Tapioca
    </div>
</footer>

</body>
</html>
